{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm -rf *.o *.dSYM/ 01_pod_vector 02_pod_matrix_auto 03_pod_matrix_rowmajor 04_pod_matrix_colmajor 05_matrix_class 06_matrix_vector 07_matrix_matrix 08_gesv 09_geev 10_syev 11_gesvd\r\n"
     ]
    }
   ],
   "source": [
    "!make clean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Matrix operations\n",
    "\n",
    "Matrices are everywhere in numerical analysis.  Arrays are the fundamental data structure and used for matrix-vector, matrix-matrix, and other linear algebraic operations.\n",
    "\n",
    "1. POD arrays and majoring\n",
    "   1. Vector: 1D array\n",
    "   1. Matrix: 2D array\n",
    "   1. Row- and column-majoring\n",
    "   1. A simple class for matrix\n",
    "2. Matrix-vector and matrix-matrix operations\n",
    "   1. Matrix-vector multiplication\n",
    "   1. Matrix-matrix multiplication\n",
    "3. Linear algebra\n",
    "   1. Linear system solution\n",
    "   1. Eigenvalue and singular value problems\n",
    "   1. Least square problems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# POD arrays\n",
    "\n",
    "The plain-old-data (POD) arrays are also called C-style arrays.  They are given the names because they are nothing more than just data and support no mechanism fancier than arithmetic.  We DO, oftentimes, wrap POD with fancy C++ constructs, but all the heavy-lifting numerical calculations still need to be done with POD.  That's how von Neumann computers work.  (It clearly reveals itself in the machine code.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vector: 1D array\n",
    "\n",
    "A vector is stored as a (usually contiguous) memory buffer of sequetially ordered elements.\n",
    "\n",
    "```cpp\n",
    "constexpr size_t width = 5;\n",
    "\n",
    "double vector[width];\n",
    "\n",
    "// Populate a vector.\n",
    "for (size_t i=0; i<width; ++i)\n",
    "{\n",
    "    vector[i] = i;\n",
    "}\n",
    "\n",
    "std::cout << \"vector elements in memory:\" << std::endl << \" \";\n",
    "for (size_t i=0; i<width; ++i)\n",
    "{\n",
    "    std::cout << \" \" << vector[i];\n",
    "}\n",
    "std::cout << std::endl;\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "g++  -std=c++17 -O3 -g -m64 -I/opt/intel/mkl/include /opt/intel/mkl/lib/libmkl_intel_lp64.a /opt/intel/mkl/lib/libmkl_sequential.a /opt/intel/mkl/lib/libmkl_core.a -lpthread -lm -ldl  -o 01_pod_vector 01_pod_vector.cpp\n",
      "vector elements in memory:\n",
      "  0 1 2 3 4\n"
     ]
    }
   ],
   "source": [
    "!make 01_pod_vector; ./01_pod_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BLAS\n",
    "\n",
    "BLAS (basic linear albegra subprograms; a standard set of array manipulation API) defines vector operations as the 1st level.  A partial list of them:\n",
    "\n",
    "* `SAXPY`: $\\mathbf{y} = a\\mathbf{x} + \\mathbf{y}$, constant times a vector plus a vector\n",
    "* `SDOT`: $\\mathbf{x}\\cdot\\mathbf{y}$, dot product of two vectors.\n",
    "* `SNRM2`: $\\sqrt{\\mathbf{y}\\cdot\\mathbf{y}}, $Euclidean norm.\n",
    "\n",
    "The operations are simple enough that we usually don't need to call the library."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matrix: 2D array\n",
    "\n",
    "See how to represent a $5\\times5$ square matrix:\n",
    "\n",
    "\\begin{align*}\n",
    "\\mathrm{A} = \\left[ a_{ij} \\right] = \\left(\\begin{array}{ccccc}\n",
    "a_{11} & a_{12} & a_{13} & a_{14} & a_{15} \\\\\n",
    "a_{21} & a_{22} & a_{23} & a_{24} & a_{25} \\\\\n",
    "a_{31} & a_{32} & a_{33} & a_{34} & a_{35} \\\\\n",
    "a_{41} & a_{42} & a_{43} & a_{44} & a_{45} \\\\\n",
    "a_{51} & a_{52} & a_{53} & a_{54} & a_{55}\n",
    "\\end{array}\\right)\n",
    "\\end{align*}\n",
    "\n",
    "$i$ is the row index (in the horizontal direction).  $j$ is the column index (in the vertical direction).  If we want to use the 0-based index, it can be rewritten as:\n",
    "\n",
    "\\begin{align*}\n",
    "\\mathrm{A} = \\left[ a_{ij} \\right] = \\left(\\begin{array}{ccccc}\n",
    "a_{00} & a_{01} & a_{02} & a_{03} & a_{04} \\\\\n",
    "a_{10} & a_{11} & a_{12} & a_{13} & a_{14} \\\\\n",
    "a_{20} & a_{21} & a_{22} & a_{23} & a_{24} \\\\\n",
    "a_{30} & a_{31} & a_{32} & a_{33} & a_{34} \\\\\n",
    "a_{40} & a_{41} & a_{42} & a_{43} & a_{44}\n",
    "\\end{array}\\right)\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In C++ we can use an auto variable like below for the matrix:\n",
    "\n",
    "```cpp\n",
    "constexpr size_t width = 5;\n",
    "\n",
    "double amatrix[width][width];\n",
    "\n",
    "// Populate the matrix on stack (row-major 2D array).\n",
    "for (size_t i=0; i<width; ++i) // the i-th row\n",
    "{\n",
    "    for (size_t j=0; j<width; ++j) // the j-th column\n",
    "    {\n",
    "        amatrix[i][j] = i*10 + j;\n",
    "    }\n",
    "}\n",
    "\n",
    "std::cout << \"2D array elements:\";\n",
    "for (size_t i=0; i<width; ++i)\n",
    "{\n",
    "    std::cout << std::endl << \" \";\n",
    "    for (size_t j=0; j<width; ++j)\n",
    "    {\n",
    "        std::cout << \" \" << std::setfill('0') << std::setw(2)\n",
    "                  << amatrix[i][j];\n",
    "    }\n",
    "}\n",
    "std::cout << std::endl;\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "g++  -std=c++17 -O3 -g -m64 -I/opt/intel/mkl/include /opt/intel/mkl/lib/libmkl_intel_lp64.a /opt/intel/mkl/lib/libmkl_sequential.a /opt/intel/mkl/lib/libmkl_core.a -lpthread -lm -ldl  -o 02_pod_matrix_auto 02_pod_matrix_auto.cpp\n",
      "2D array elements:\n",
      "  00 01 02 03 04\n",
      "  10 11 12 13 14\n",
      "  20 21 22 23 24\n",
      "  30 31 32 33 34\n",
      "  40 41 42 43 44\n"
     ]
    }
   ],
   "source": [
    "!make 02_pod_matrix_auto; ./02_pod_matrix_auto"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The C++ multi-dimensional array index is convenient, but it doesn't always work when the array size isn't known in the compile time.  `g++` accepts the following code, but `clang++` doesn't.\n",
    "\n",
    "```cpp\n",
    "void work(double * buffer, size_t width)\n",
    "{\n",
    "    // This won't work since width isn't known in compile time.\n",
    "    double (*matrix)[width] = reinterpret_cast<double (*)[width]>(buffer);\n",
    "    \n",
    "    //...\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "g++  -std=c++17 -O3 -g -m64 -I/opt/intel/mkl/include /opt/intel/mkl/lib/libmkl_intel_lp64.a /opt/intel/mkl/lib/libmkl_sequential.a /opt/intel/mkl/lib/libmkl_core.a -lpthread -lm -ldl  -o pod_bad_matrix pod_bad_matrix.cpp\n",
      "\u001b[1mpod_bad_matrix.cpp:7:14: \u001b[0m\u001b[0;1;31merror: \u001b[0m\u001b[1mcannot initialize a variable of type\n",
      "      'double (*)[width]' with an rvalue of type 'double (*)[width]'\u001b[0m\n",
      "    double (*matrix)[width] = reinterpret_cast<double (*)[width]>(buffer);\n",
      "\u001b[0;1;32m             ^                ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "\u001b[0m1 error generated.\n",
      "make: *** [pod_bad_matrix] Error 1\n"
     ]
    }
   ],
   "source": [
    "# g++ can build; clang++ errors out\n",
    "!make pod_bad_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Row-major 2D variable-size array\n",
    "\n",
    "The elements of a row-major 2D array are stored so that the fastest changing index is the trailing index of the 2D array:\n",
    "\n",
    "\\begin{align*}\n",
    "\\mathrm{buffer} = [a_{00}, a_{01}, a_{02}, a_{03}, a_{04}, a_{10}, a_{11}, a_{12}, \\ldots, a_{43}, a_{44}]\n",
    "\\end{align*}\n",
    "\n",
    "When accessing the elements, all we need to do is to remember how long we need to *stride* per row (leading) index.\n",
    "\n",
    "```cpp\n",
    "constexpr size_t width = 5;\n",
    "\n",
    "double * buffer = new double[width*width];\n",
    "double (*matrix)[width] = reinterpret_cast<double (*)[width]>(buffer);\n",
    "std::cout << \"buffer address: \" << buffer << std::endl\n",
    "          << \"matrix address: \" << matrix << std::endl;\n",
    "\n",
    "// Populate a buffer (row-major 2D array).\n",
    "for (size_t i=0; i<width; ++i) // the i-th row\n",
    "{\n",
    "    for (size_t j=0; j<width; ++j) // the j-th column\n",
    "    {\n",
    "        buffer[i*width + j] = i*10 + j;\n",
    "    }\n",
    "}\n",
    "\n",
    "std::cout << \"matrix (row-major) elements as 2D array:\";\n",
    "for (size_t i=0; i<width; ++i)\n",
    "{\n",
    "    std::cout << std::endl << \" \";\n",
    "    for (size_t j=0; j<width; ++j)\n",
    "    {\n",
    "        std::cout << \" \" << std::setfill('0') << std::setw(2)\n",
    "                  << matrix[i][j];\n",
    "    }\n",
    "}\n",
    "std::cout << std::endl;\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "g++  -std=c++17 -O3 -g -m64 -I/opt/intel/mkl/include /opt/intel/mkl/lib/libmkl_intel_lp64.a /opt/intel/mkl/lib/libmkl_sequential.a /opt/intel/mkl/lib/libmkl_core.a -lpthread -lm -ldl  -o 03_pod_matrix_rowmajor 03_pod_matrix_rowmajor.cpp\n",
      "buffer address: 0x7fc285c018c0\n",
      "matrix address: 0x7fc285c018c0\n",
      "matrix (row-major) elements as 2D array:\n",
      "  00 01 02 03 04\n",
      "  10 11 12 13 14\n",
      "  20 21 22 23 24\n",
      "  30 31 32 33 34\n",
      "  40 41 42 43 44\n",
      "matrix (row-major) elements in memory:\n",
      "  00 01 02 03 04 10 11 12 13 14 20 21 22 23 24 30 31 32 33 34 40 41 42 43 44\n",
      "row majoring: the fastest moving index is the trailing index\n"
     ]
    }
   ],
   "source": [
    "!make 03_pod_matrix_rowmajor; ./03_pod_matrix_rowmajor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Column-major 2D variable-size array\n",
    "\n",
    "The elements of a column-major 2D array are stored so that the fastest changing index is the leading index of the 2D array:\n",
    "\n",
    "\\begin{align*}\n",
    "\\mathrm{buffer} = [a_{00}, a_{10}, a_{20}, a_{30}, a_{40}, a_{01}, a_{11}, a_{21}, \\ldots, a_{34}, a_{44}]\n",
    "\\end{align*}\n",
    "\n",
    "Similar to a row-major array, we need to know the stride.  But this time it's for the column (trailing) index.\n",
    "\n",
    "```cpp\n",
    "constexpr size_t width = 5;\n",
    "\n",
    "double * buffer = new double[width*width];\n",
    "double (*matrix)[width] = reinterpret_cast<double (*)[width]>(buffer);\n",
    "std::cout << \"buffer address: \" << buffer << std::endl\n",
    "          << \"matrix address: \" << matrix << std::endl;\n",
    "\n",
    "// Populate a buffer (column-major 2D array).\n",
    "for (size_t i=0; i<width; ++i) // the i-th row\n",
    "{\n",
    "    for (size_t j=0; j<width; ++j) // the j-th column\n",
    "    {\n",
    "        buffer[j*width + i] = i*10 + j;\n",
    "    }\n",
    "}\n",
    "\n",
    "std::cout << \"matrix (column-major) elements as 2D array:\";\n",
    "for (size_t i=0; i<width; ++i)\n",
    "{\n",
    "    std::cout << std::endl << \" \";\n",
    "    for (size_t j=0; j<width; ++j)\n",
    "    {\n",
    "        std::cout << \" \" << std::setfill('0') << std::setw(2)\n",
    "                  << matrix[j][i];\n",
    "    }\n",
    "}\n",
    "std::cout << std::endl;\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "g++  -std=c++17 -O3 -g -m64 -I/opt/intel/mkl/include /opt/intel/mkl/lib/libmkl_intel_lp64.a /opt/intel/mkl/lib/libmkl_sequential.a /opt/intel/mkl/lib/libmkl_core.a -lpthread -lm -ldl  -o 04_pod_matrix_colmajor 04_pod_matrix_colmajor.cpp\n",
      "buffer address: 0x7f8f564018c0\n",
      "matrix address: 0x7f8f564018c0\n",
      "matrix (column-major) elements as 2D array:\n",
      "  00 01 02 03 04\n",
      "  10 11 12 13 14\n",
      "  20 21 22 23 24\n",
      "  30 31 32 33 34\n",
      "  40 41 42 43 44\n",
      "matrix (column-major) elements in memory:\n",
      "  00 10 20 30 40 01 11 21 31 41 02 12 22 32 42 03 13 23 33 43 04 14 24 34 44\n",
      "column majoring: the fastest moving index is the leading index\n"
     ]
    }
   ],
   "source": [
    "!make 04_pod_matrix_colmajor; ./04_pod_matrix_colmajor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## C++ class to treat the memory buffer like matrix\n",
    "\n",
    "Keeping track of the stride can be error-prone.  Even if we stick to one majoring order (usually it's row-majoring), it's easy to lose track of it when the number of row and column is different, or it's higher-dimensional.\n",
    "\n",
    "A common practice in C++ is to use a class to keep track of the stride.  Properly defined accessors significantly simplifies it.\n",
    "\n",
    "```cpp\n",
    "class Matrix {\n",
    "\n",
    "public:\n",
    "\n",
    "    Matrix(size_t nrow, size_t ncol)\n",
    "      : m_nrow(nrow), m_ncol(ncol)\n",
    "    {\n",
    "        size_t nelement = nrow * ncol;\n",
    "        m_buffer = new double[nelement];\n",
    "    }\n",
    "\n",
    "    // TODO: copy and move constructors and assignment operators.\n",
    "\n",
    "    ~Matrix()\n",
    "    {\n",
    "        delete[] m_buffer;\n",
    "    }\n",
    "\n",
    "    // No bound check.\n",
    "    double   operator() (size_t row, size_t col) const { return m_buffer[row*m_ncol + col]; }\n",
    "    double & operator() (size_t row, size_t col)       { return m_buffer[row*m_ncol + col]; }\n",
    "\n",
    "    size_t nrow() const { return m_nrow; }\n",
    "    size_t ncol() const { return m_ncol; }\n",
    "\n",
    "private:\n",
    "\n",
    "    size_t m_nrow;\n",
    "    size_t m_ncol;\n",
    "    double * m_buffer;\n",
    "\n",
    "};\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "g++  -std=c++17 -O3 -g -m64 -I/opt/intel/mkl/include /opt/intel/mkl/lib/libmkl_intel_lp64.a /opt/intel/mkl/lib/libmkl_sequential.a /opt/intel/mkl/lib/libmkl_core.a -lpthread -lm -ldl  -o 05_matrix_class 05_matrix_class.cpp\n",
      "matrix:\n",
      "  00 01 02 03 04\n",
      "  10 11 12 13 14\n",
      "  20 21 22 23 24\n",
      "  30 31 32 33 34\n",
      "  40 41 42 43 44\n"
     ]
    }
   ],
   "source": [
    "!make 05_matrix_class; ./05_matrix_class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Matrix-vector multiplication"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BLAS level 2 includes matrix-vector operations.\n",
    "\n",
    "Operations of a matrix and a vector is much more interesting than vector operations.  What we really need to do is the matrix-vector multiplication\n",
    "\n",
    "\\begin{align*}\n",
    "\\mathbf{y} = \\mathrm{A}\\mathbf{x}\n",
    "\\end{align*}\n",
    "\n",
    "But because a matrix is a 2D array, we should first discuss traspose.  Write a $m\\times n$ ($m$ rows and $n$ columns) matrix $\\mathrm{A}$\n",
    "\n",
    "\\begin{align*}\n",
    "\\mathrm{A} = [a_{ij}] = \\left(\\begin{array}{cccc}\n",
    "a_{11} & a_{12} & \\cdots & a_{1n} \\\\\n",
    "a_{21} & a_{22} & \\cdots & a_{2n} \\\\\n",
    "a_{31} & a_{32} & \\cdots & a_{3n} \\\\\n",
    "\\vdots & & \\ddots & \\vdots \\\\\n",
    "a_{m1} & a_{m2} & \\cdots & a_{mn}\n",
    "\\end{array}\\right)_{m\\times n}\n",
    "\\end{align*}\n",
    "\n",
    "its transpose $\\mathrm{A}^t$ becomes a $n\\times m$ ($n$ rows and $m$ columns) matrix\n",
    "\n",
    "\\begin{align*}\n",
    "\\mathrm{A}^t = [a_{ji}] = \\left(\\begin{array}{ccccc}\n",
    "a_{11} & a_{21} & a_{31} & \\cdots & a_{m1} \\\\\n",
    "a_{12} & a_{22} & a_{32} & \\cdots & a_{m2} \\\\\n",
    "\\vdots & & & \\ddots & \\vdots \\\\\n",
    "a_{1n} & a_{2n} & a_{3n} & \\cdots & a_{mn}\n",
    "\\end{array}\\right)_{n\\times m}\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fast transpose can be done by taking advantage of majoring.  The key is the formula $\\mathrm{A}^t = [a_{ji}]$ for $\\mathrm{A} = [a_{ij}]$.  The code is like:\n",
    "\n",
    "```cpp\n",
    "double   operator() (size_t row, size_t col) const { return m_buffer[index(row, col)]; }\n",
    "double & operator() (size_t row, size_t col)       { return m_buffer[index(row, col)]; }\n",
    "\n",
    "bool is_transposed() const { return m_transpose; }\n",
    "\n",
    "Matrix & transpose()\n",
    "{\n",
    "    m_transpose = !m_transpose;\n",
    "    std::swap(m_nrow, m_ncol);\n",
    "    return *this;\n",
    "}\n",
    "```\n",
    "\n",
    "There is no data copied for transpose.  The price to pay is the if statement in the indexing helper.\n",
    "\n",
    "```cpp\n",
    "size_t index(size_t row, size_t col) const\n",
    "{\n",
    "    if (m_transpose) { return row          + col * m_nrow; }\n",
    "    else             { return row * m_ncol + col         ; }\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Come back to the matrix-vector multiplication, $\\mathbf{y} = \\mathrm{A}\\mathbf{x}$.  The calculation is easy by using the index form of the matrix and vector.\n",
    "\n",
    "\\begin{align*}\n",
    "y_i = \\sum_{j=1}^n A_{ij} x_j, \\quad i = 1, \\ldots, m\n",
    "\\end{align*}\n",
    "\n",
    "Sometimes, when Einstein's summation convention is applied, the summation sign may be suppressed, and the repeated indices imply summation\n",
    "\n",
    "\\begin{align*}\n",
    "y_i = A_{ij} x_j, \\quad i = 1, \\ldots, m, \\; j = 1, \\ldots, n\n",
    "\\end{align*}\n",
    "\n",
    "It can be shown that the index form of $\\mathbf{y}' = \\mathrm{A}^t\\mathbf{x}'$ is\n",
    "\n",
    "\\begin{align*}\n",
    "y'_i = A_{ji} x'_j, \\quad j = 1, \\ldots, m, \\; i = 1, \\ldots, n\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement a naive matrix-vector multiplication:\n",
    "\n",
    "```cpp\n",
    "std::vector<double> operator*(Matrix const & mat, std::vector<double> const & vec)\n",
    "{\n",
    "    if (mat.ncol() != vec.size())\n",
    "    {\n",
    "        throw std::out_of_range(\"matrix column differs from vector size\");\n",
    "    }\n",
    "\n",
    "    std::vector<double> ret(mat.nrow());\n",
    "\n",
    "    for (size_t i=0; i<mat.nrow(); ++i)\n",
    "    {\n",
    "        double v = 0;\n",
    "        for (size_t j=0; j<mat.ncol(); ++j)\n",
    "        {\n",
    "            v += mat(i,j) * vec[j];\n",
    "        }\n",
    "        ret[i] = v;\n",
    "    }\n",
    "\n",
    "    return ret;\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "g++  -std=c++17 -O3 -g -m64 -I/opt/intel/mkl/include /opt/intel/mkl/lib/libmkl_intel_lp64.a /opt/intel/mkl/lib/libmkl_sequential.a /opt/intel/mkl/lib/libmkl_core.a -lpthread -lm -ldl  -o 06_matrix_vector 06_matrix_vector.cpp\n",
      ">>> square matrix-vector multiplication:\n",
      "matrix A:\n",
      "   1  0  0  0  0\n",
      "   0  1  0  0  0\n",
      "   0  0  1  0  0\n",
      "   0  0  0  1  0\n",
      "   0  0  0  0  1\n",
      "vector b: 1 0 0 0 0\n",
      "A*b = 1 0 0 0 0\n",
      ">>> m*n matrix-vector multiplication:\n",
      "matrix A:\n",
      "   1  2  3\n",
      "   4  5  6\n",
      "vector b: 1 2 3\n",
      "A*b = 14 32\n",
      ">>> transposed matrix-vector multiplication:\n",
      "matrix A:\n",
      "   1  4\n",
      "   2  5\n",
      "   3  6\n",
      "matrix A buffer: 1 2 3 4 5 6\n",
      "vector b: 1 2\n",
      "A*b = 9 12 15\n",
      ">>> copied transposed matrix-vector multiplication:\n",
      "matrix A:\n",
      "   1  4\n",
      "   2  5\n",
      "   3  6\n",
      "matrix A buffer: 1 4 2 5 3 6\n",
      "vector b: 1 2\n",
      "A*b = 9 12 15\n"
     ]
    }
   ],
   "source": [
    "!make 06_matrix_vector; ./06_matrix_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The majoring may significantly affects the speed of matrix-vector multiplication."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Matrix-matrix multiplication"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BLAS level 3 includes matrix-matrix operations.\n",
    "\n",
    "Matrix-matrix multiplication, $\\mathrm{C} = \\mathrm{A}\\mathrm{B}$ generally uses a $O(n^3)$ algorithm for $O(n^2)$ data.  The formula is\n",
    "\n",
    "\\begin{align*}\n",
    "C_{ik} = \\sum_{j=1}^n A_{ij}B_{jk}, \\quad i = 1, \\ldots, m, \\; k = 1, \\ldots, l\n",
    "\\end{align*}\n",
    "\n",
    "or, by using Einstein's summation convention,\n",
    "\n",
    "\\begin{align*}\n",
    "C_{ik} = A_{ij}B_{jk}, \\quad i = 1, \\ldots, m, \\; j = 1, \\ldots, n, \\; k = 1, \\ldots, l\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A naive C++ implementation:\n",
    "\n",
    "```cpp\n",
    "Matrix operator*(Matrix const & mat1, Matrix const & mat2)\n",
    "{\n",
    "    if (mat1.ncol() != mat2.nrow())\n",
    "    {\n",
    "        throw std::out_of_range(\n",
    "            \"the number of first matrix column \"\n",
    "            \"differs from that of second matrix row\");\n",
    "    }\n",
    "\n",
    "    Matrix ret(mat1.nrow(), mat2.ncol());\n",
    "\n",
    "    for (size_t i=0; i<ret.nrow(); ++i)\n",
    "    {\n",
    "        for (size_t k=0; k<ret.ncol(); ++k)\n",
    "        {\n",
    "            double v = 0;\n",
    "            for (size_t j=0; j<mat1.ncol(); ++j)\n",
    "            {\n",
    "                v += mat1(i,j) * mat2(j,k);\n",
    "            }\n",
    "            ret(i,k) = v;\n",
    "        }\n",
    "    }\n",
    "\n",
    "    return ret;\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "g++  -std=c++17 -O3 -g -m64 -I/opt/intel/mkl/include /opt/intel/mkl/lib/libmkl_intel_lp64.a /opt/intel/mkl/lib/libmkl_sequential.a /opt/intel/mkl/lib/libmkl_core.a -lpthread -lm -ldl  -o 07_matrix_matrix 07_matrix_matrix.cpp\n",
      ">>> A(2x3) times B(3x2):\n",
      "matrix A (2x3):\n",
      "   1  2  3\n",
      "   4  5  6\n",
      "matrix B (3x2):\n",
      "   1  2\n",
      "   3  4\n",
      "   5  6\n",
      "result matrix C (2x2) = AB:\n",
      "  22 28\n",
      "  49 64\n",
      ">>> B(3x2) times A(2x3):\n",
      "matrix B (3x2):\n",
      "   1  2\n",
      "   3  4\n",
      "   5  6\n",
      "matrix A (2x3):\n",
      "   1  2  3\n",
      "   4  5  6\n",
      "result matrix D (3x3) = BA:\n",
      "   9 12 15\n",
      "  19 26 33\n",
      "  29 40 51\n"
     ]
    }
   ],
   "source": [
    "!make 07_matrix_matrix; ./07_matrix_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Matrix-matrix multiplication is intensive number-crunching.  The naive, brute-force, n-cube algorithm is basically what we need to do, without a way around.\n",
    "\n",
    "It also demands memory.  A matrix of $100,000\\times100,000$ takes $10,000,000,000$ (i.e., $10^{10}$) elements, and with double-precision floating points, it takes 80 GB.  To perform multiplication, you need the memory for 3 of the matrices, and that's 240 GB.  The dense matrix multiplication generally cannot use distributed memory without significantly loss of runtime speed.  The reasonable size of dense matrices for a workstation is around $10,000\\times10,000$, i.e., 800 MB per matrix.  It's very limiting, but already facilitates a good number of applications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear algebra"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the matrix operations, it is time to introduce linear algebra in C++.  There are two critically important software packages: BLAS (http://www.netlib.org/blas/) and LAPACK (http://www.netlib.org/lapack/).  They were developed in FORTRAN.  Although the FORTRAN code is still being maintained today, it serves more like a reference implementation.  Multiple vendors provide optimized implementation, e.g., Intel's Math Kernel Library (MKL), Apple's vecLib, etc.\n",
    "\n",
    "BLAS stands for Basic Linear Algebra Subprograms, and LAPACK is Linear Algebra PACKage.  LAPACK is designed to rely on the underneath BLAS, so the two libraries are usually used together.  For example, the general matrix-vector multiplication is defined as the `?GEMV` function in BLAS level 2 (`?` can be one of `S`, `D`, `C`, and `Z`, for single-precision real, double-precision real, single-precision complex, and double-precision complex, respectively), and the general matrix-matrix multiplication is the `?GEMM` function in BLAS level 3.\n",
    "\n",
    "While BLAS offers basic operations like matrix multiplication, LAPACK provides more versatile computation helpers or solvers, e.g., a system of linear equations, least square, and eigen problems.\n",
    "\n",
    "Both BLAS and LAPACK provide C interface.  They don't native C++ interface, but the C interface is compatible to C++.  CBLAS is the C interface for BLAS, and LAPACKE is that for LAPACK."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear system"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LAPACK provides `?GESV` functions to solve a linear system using a general (dense) matrix: $\\mathrm{A}\\mathbf{x} = \\mathbf{b}$.  Say we have a system of linear equations:\n",
    "\n",
    "\\begin{align*}\n",
    "3 x_1 + 5 x_2 + 2 x_3 &= 57 \\\\\n",
    "2 x_1 +   x_2 + 3 x_3 &= 22 \\\\\n",
    "4 x_1 + 3 x_2 + 2 x_3 &= 41\n",
    "\\end{align*}\n",
    "\n",
    "It can be rewritten as $\\mathrm{A}\\mathbf{x} = \\mathbf{b}$, where\n",
    "\n",
    "\\begin{align*}\n",
    "\\mathrm{A} = \\left(\\begin{array}{ccc}\n",
    "3 & 5 & 2 \\\\\n",
    "2 & 1 & 3 \\\\\n",
    "4 & 3 & 2\n",
    "\\end{array}\\right), \\quad\n",
    "\\mathbf{b} = \\left(\\begin{array}{c}\n",
    "57 \\\\ 22 \\\\ 41\n",
    "\\end{array}\\right), \\quad\n",
    "\\mathbf{x} = \\left(\\begin{array}{c}\n",
    "x_1 \\\\ x_2 \\\\ x_3\n",
    "\\end{array}\\right)\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the reference implementation of LAPACK is FORTRAN, which uses column major.  The dimensional arguments of the LAPACK subroutines changes meaning when we call them from C with row-major matrices.\n",
    "\n",
    "```cpp\n",
    "const size_t n = 3;\n",
    "int status;\n",
    "\n",
    "std::cout << \">>> Solve Ax=b (row major)\" << std::endl;\n",
    "Matrix mat(n, n, false);\n",
    "mat(0,0) = 3; mat(0,1) = 5; mat(0,2) = 2;\n",
    "mat(1,0) = 2; mat(1,1) = 1; mat(1,2) = 3;\n",
    "mat(2,0) = 4; mat(2,1) = 3; mat(2,2) = 2;\n",
    "Matrix b(n, 2, false);\n",
    "b(0,0) = 57; b(0,1) = 23;\n",
    "b(1,0) = 22; b(1,1) = 12;\n",
    "b(2,0) = 41; b(2,1) = 84;\n",
    "std::vector<int> ipiv(n);\n",
    "\n",
    "std::cout << \"A:\" << mat << std::endl;\n",
    "std::cout << \"b:\" << b << std::endl;\n",
    "\n",
    "status = LAPACKE_dgesv(\n",
    "    LAPACK_ROW_MAJOR // int matrix_layout\n",
    "  , n // lapack_int n\n",
    "  , b.ncol() // lapack_int nrhs\n",
    "  , mat.data() // double * a\n",
    "  , mat.ncol() // lapack_int lda\n",
    "  , ipiv.data() // lapack_int * ipiv\n",
    "  , b.data() // double * b\n",
    "  , b.ncol() // lapack_int ldb\n",
    "  // for row major matrix, ldb becomes the trailing dimension.\n",
    ");\n",
    "\n",
    "std::cout << \"solution x:\" << b << std::endl;\n",
    "std::cout << \"dgesv status: \" << status << std::endl;\n",
    "\n",
    "std::cout << \">>> Solve Ax=b (column major)\" << std::endl;\n",
    "Matrix mat2 = Matrix(n, n, true);\n",
    "mat2(0,0) = 3; mat2(0,1) = 5; mat2(0,2) = 2;\n",
    "mat2(1,0) = 2; mat2(1,1) = 1; mat2(1,2) = 3;\n",
    "mat2(2,0) = 4; mat2(2,1) = 3; mat2(2,2) = 2;\n",
    "Matrix b2(n, 2, true);\n",
    "b2(0,0) = 57; b2(0,1) = 23;\n",
    "b2(1,0) = 22; b2(1,1) = 12;\n",
    "b2(2,0) = 41; b2(2,1) = 84;\n",
    "\n",
    "std::cout << \"A:\" << mat2 << std::endl;\n",
    "std::cout << \"b:\" << b2 << std::endl;\n",
    "\n",
    "status = LAPACKE_dgesv(\n",
    "    LAPACK_COL_MAJOR // int matrix_layout\n",
    "  , n // lapack_int n\n",
    "  , b2.ncol() // lapack_int nrhs\n",
    "  , mat2.data() // double * a\n",
    "  , mat2.nrow() // lapack_int lda\n",
    "  , ipiv.data() // lapack_int * ipiv\n",
    "  , b2.data() // double * b\n",
    "  , b2.nrow() // lapack_int ldb\n",
    "  // for column major matrix, ldb remains the leading dimension.\n",
    ");\n",
    "\n",
    "std::cout << \"solution x:\" << b2 << std::endl;\n",
    "std::cout << \"dgesv status: \" << status << std::endl;\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "g++  -std=c++17 -O3 -g -m64 -I/opt/intel/mkl/include /opt/intel/mkl/lib/libmkl_intel_lp64.a /opt/intel/mkl/lib/libmkl_sequential.a /opt/intel/mkl/lib/libmkl_core.a -lpthread -lm -ldl  -o 08_gesv 08_gesv.cpp\n",
      ">>> Solve Ax=b (row major)\n",
      "A:\n",
      "   3  5  2\n",
      "   2  1  3\n",
      "   4  3  2\n",
      " data:   3  5  2  2  1  3  4  3  2\n",
      "b:\n",
      "  57 23\n",
      "  22 12\n",
      "  41 84\n",
      " data:  57 23 22 12 41 84\n",
      "solution x:\n",
      "   2 38.3913\n",
      "   9 -11.3043\n",
      "   3 -17.8261\n",
      " data:   2 38.3913  9 -11.3043  3 -17.8261\n",
      "dgesv status: 0\n",
      ">>> Solve Ax=b (column major)\n",
      "A:\n",
      "   3  5  2\n",
      "   2  1  3\n",
      "   4  3  2\n",
      " data:   3  2  4  5  1  3  2  3  2\n",
      "b:\n",
      "  57 23\n",
      "  22 12\n",
      "  41 84\n",
      " data:  57 22 41 23 12 84\n",
      "solution x:\n",
      "   2 38.3913\n",
      "   9 -11.3043\n",
      "   3 -17.8261\n",
      " data:   2  9  3 38.3913 -11.3043 -17.8261\n",
      "dgesv status: 0\n"
     ]
    }
   ],
   "source": [
    "!make 08_gesv; ./08_gesv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Eigenvalue problems and SVD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Eigenvalue problems and SVD are popular ways to factorize matrices.  The eigenvalue problems are to find the eigenvalues $\\lambda_1, \\lambda_2, \\ldots, \\lambda_n$ and the eigenvector matrix $\\mathrm{S}$ of a matrix $\\mathrm{A}$, such that\n",
    "\n",
    "\\begin{align*}\n",
    "\\mathrm{A} = \\mathrm{S}\\mathrm{\\Lambda}\\mathrm{S}^{-1}\n",
    "\\end{align*}\n",
    "\n",
    "An eigenvalue $\\lambda$ of $\\mathrm{A}$ is a scalar such that\n",
    "\n",
    "\\begin{align*}\n",
    "\\mathrm{A}v = \\lambda v\n",
    "\\end{align*}\n",
    "\n",
    "$v$ is an eigenvector associated with $\\lambda$.  Because $v$ is after $\\mathrm{A}$, it is also called right eigenvector.  For the same eigenvalue $\\lambda$, the left eigenvector can be found by the following equation\n",
    "\n",
    "\\begin{align*}\n",
    "u^h\\mathrm{A} = \\lambda u^h\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use LAPACK's high-level `?GEEV` driver for calculating the eigenvalues and eigenvectors:\n",
    "\n",
    "```cpp\n",
    "const size_t n = 3;\n",
    "int status;\n",
    "\n",
    "std::cout << \">>> Solve Ax=lx (row major)\" << std::endl;\n",
    "Matrix mat(n, n, false);\n",
    "mat(0,0) = 3; mat(0,1) = 5; mat(0,2) = 2;\n",
    "mat(1,0) = 2; mat(1,1) = 1; mat(1,2) = 3;\n",
    "mat(2,0) = 4; mat(2,1) = 3; mat(2,2) = 2;\n",
    "std::vector<double> wr(n), wi(n);\n",
    "Matrix vl(n, n, false), vr(n, n, false);\n",
    "\n",
    "std::vector<int> ipiv(n);\n",
    "\n",
    "std::cout << \"A:\" << mat << std::endl;\n",
    "\n",
    "status = LAPACKE_dgeev(\n",
    "    LAPACK_ROW_MAJOR // int matrix_layout\n",
    "  , 'V' // char jobvl; 'V' to compute left eigenvectors, 'N' to not compute them\n",
    "  , 'V' // char jobvr; 'V' to compute right eigenvectors, 'N' to not compute them\n",
    "  , n // lapack_int n\n",
    "  , mat.data() // double * a\n",
    "  , mat.ncol() // lapack_int lda\n",
    "  , wr.data() // double * wr\n",
    "  , wi.data() // double * wi\n",
    "  , vl.data() // double * vl\n",
    "  , vl.ncol() // lapack_int ldvl\n",
    "  , vr.data() // double * vr\n",
    "  , vr.ncol() // lapack_int ldvr\n",
    ");\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "g++  -std=c++17 -O3 -g -m64 -I/opt/intel/mkl/include /opt/intel/mkl/lib/libmkl_intel_lp64.a /opt/intel/mkl/lib/libmkl_sequential.a /opt/intel/mkl/lib/libmkl_core.a -lpthread -lm -ldl  -o 09_geev 09_geev.cpp\n",
      ">>> Solve Ax=lx (row major)\n",
      "A:\n",
      "   3  5  2\n",
      "   2  1  3\n",
      "   4  3  2\n",
      " data:   3  5  2  2  1  3  4  3  2\n",
      "dgeev status: 0\n",
      "eigenvalues:\n",
      "      (real)      (imag)\n",
      "(   8.270757,   0.000000)\n",
      "(  -1.135379,   1.221392)\n",
      "(  -1.135379,  -1.221392)\n",
      "left eigenvectors:\n",
      "    0.609288 ( -0.012827, -0.425749) ( -0.012827,  0.425749)\n",
      "    0.621953 (  0.652142,  0.000000) (  0.652142,  0.000000)\n",
      "    0.491876 ( -0.442811,  0.444075) ( -0.442811, -0.444075)\n",
      "right eigenvectors:\n",
      "    0.649714 ( -0.668537,  0.000000) ( -0.668537,  0.000000)\n",
      "    0.435736 (  0.448552, -0.330438) (  0.448552,  0.330438)\n",
      "    0.622901 (  0.260947,  0.417823) (  0.260947, -0.417823)\n"
     ]
    }
   ],
   "source": [
    "!make 09_geev; ./09_geev"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verify the calculation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Verify the left eigenvector with the first eigenvalue:\n",
      "  x^t A: [5.039274 5.144021 4.068187]\n",
      "  l x^t: [5.03927482 5.14402399 4.06818835]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "A = np.array([[3, 5, 2], [2, 1, 3], [4, 3, 2]], dtype='float64')\n",
    "\n",
    "xl = np.array([0.609288, 0.621953, 0.491876], dtype='float64')\n",
    "print(\"Verify the left eigenvector with the first eigenvalue:\")\n",
    "print(\"  x^t A:\", np.dot(xl, A))\n",
    "print(\"  l x^t:\", 8.27076*xl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Verify the left eigenvector with the second eigenvalue (complex-valued):\n",
      "  x^h A: [-0.50544107-0.49905294j -0.74042605+0.79652005j  1.04514994-0.03665197j]\n",
      "  l x^h: [-0.5054429 -0.49905324j -0.74042827+0.796521j    1.0451479 -0.03665245j]\n"
     ]
    }
   ],
   "source": [
    "print(\"Verify the left eigenvector with the second eigenvalue (complex-valued):\")\n",
    "\n",
    "xl = np.array([-0.012827-0.425749j, 0.652142, -0.442811+0.444075j], dtype='complex64')\n",
    "# NOTE: the left eigenvector needs the conjugate.\n",
    "print(\"  x^h A:\", np.dot(xl.conj(), A))\n",
    "print(\"  l x^h:\", (-1.135379+1.221392j)*xl.conj())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Verify the right eigenvector with the first eigenvalue:\n",
      "  A x: [5.373624 3.603867 5.151866]\n",
      "  l x: [5.37362856 3.60386788 5.15186467]\n"
     ]
    }
   ],
   "source": [
    "print(\"Verify the right eigenvector with the first eigenvalue:\")\n",
    "\n",
    "xr = np.array([0.649714, 0.435736, 0.622901], dtype='float64')\n",
    "print(\"  A x:\", np.dot(A, xr))\n",
    "print(\"  l x:\", 8.27076*xr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Verify the right eigenvector with the second eigenvalue (complex-valued):\n",
      "  A x: [ 0.75904298-0.81654397j -0.10568106+0.92303097j -0.80659807-0.15566799j]\n",
      "  l x: [ 0.75904286-0.8165458j  -0.10568219+0.92303026j -0.8065994 -0.15566885j]\n"
     ]
    }
   ],
   "source": [
    "print(\"Verify the right eigenvector with the second eigenvalue (complex-valued):\")\n",
    "\n",
    "xr = np.array([-0.668537, 0.448552-0.330438j, 0.260947+0.417823j], dtype='complex64')\n",
    "print(\"  A x:\", np.dot(A, xr))\n",
    "print(\"  l x:\", (-1.135379+1.221392j)*xr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LAPACK's `?SYEV` calculates the eigenvalues and eigenvectors for symmetric matrices.\n",
    "\n",
    "```cpp\n",
    "const size_t n = 3;\n",
    "int status;\n",
    "\n",
    "std::cout << \">>> Solve Ax=lx (row major, A symmetrix)\" << std::endl;\n",
    "Matrix mat(n, n, false);\n",
    "mat(0,0) = 3; mat(0,1) = 5; mat(0,2) = 2;\n",
    "mat(1,0) = 5; mat(1,1) = 1; mat(1,2) = 3;\n",
    "mat(2,0) = 2; mat(2,1) = 3; mat(2,2) = 2;\n",
    "std::vector<double> w(n);\n",
    "\n",
    "std::cout << \"A:\" << mat << std::endl;\n",
    "\n",
    "status = LAPACKE_dsyev(\n",
    "    LAPACK_ROW_MAJOR // int matrix_layout\n",
    "  , 'V' // char jobz; 'V' to compute both eigenvalues and eigenvectors, 'N' only eigenvalues\n",
    "  , 'U' // char uplo; 'U' use the upper triangular of input a, 'L' use the lower\n",
    "  , n // lapack_int n\n",
    "  , mat.data() // double * a\n",
    "  , mat.ncol() // lapack_int lda\n",
    "  , w.data() // double * w\n",
    ");\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "g++  -std=c++17 -O3 -g -m64 -I/opt/intel/mkl/include /opt/intel/mkl/lib/libmkl_intel_lp64.a /opt/intel/mkl/lib/libmkl_sequential.a /opt/intel/mkl/lib/libmkl_core.a -lpthread -lm -ldl  -o 10_syev 10_syev.cpp\n",
      ">>> Solve Ax=lx (row major, A symmetrix)\n",
      "A:\n",
      "   3  5  2\n",
      "   5  1  3\n",
      "   2  3  2\n",
      " data:   3  5  2  5  1  3  2  3  2\n",
      "dsyev status: 0\n",
      "eigenvalues:  -3.36105 0.503874 8.85717\n",
      "eigenvectors:\n",
      "  -0.551825 -0.505745 -0.663107\n",
      "  0.798404 -0.0906812 -0.595255\n",
      "  -0.240916 0.857904 -0.453828\n",
      " data:  -0.551825 -0.505745 -0.663107 0.798404 -0.0906812 -0.595255 -0.240916 0.857904 -0.453828\n"
     ]
    }
   ],
   "source": [
    "!make 10_syev; ./10_syev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Verify the right eigenvectors with the eigenvalues:\n",
      "First eigenvalue:\n",
      "  A x: [ 1.854713 -2.683469  0.80973 ]\n",
      "  l x: [ 1.85471142 -2.68347576  0.80973072]\n",
      "Second eigenvalue:\n",
      "  A x: [-0.254833  -0.0456942  0.4322744]\n",
      "  l x: [-0.25483176 -0.0456919   0.43227552]\n",
      "Third eigenvalue:\n",
      "  A x: [-5.873252 -5.272274 -4.019635]\n",
      "  l x: [-5.87325143 -5.27227473 -4.01963175]\n"
     ]
    }
   ],
   "source": [
    "print(\"Verify the right eigenvectors with the eigenvalues:\")\n",
    "\n",
    "A = np.array([[3, 5, 2], [5, 1, 3], [2, 3, 2]], dtype='float64')\n",
    "\n",
    "print(\"First eigenvalue:\")\n",
    "x = np.array([-0.551825, 0.798404, -0.240916], dtype='float64')\n",
    "print(\"  A x:\", np.dot(A, x))\n",
    "print(\"  l x:\", -3.36105*x)\n",
    "\n",
    "print(\"Second eigenvalue:\")\n",
    "x = np.array([-0.505745, -0.0906812, 0.857904], dtype='float64')\n",
    "print(\"  A x:\", np.dot(A, x))\n",
    "print(\"  l x:\", 0.503874*x)\n",
    "\n",
    "print(\"Third eigenvalue:\")\n",
    "x = np.array([-0.663107, -0.595255, -0.453828], dtype='float64')\n",
    "print(\"  A x:\", np.dot(A, x))\n",
    "print(\"  l x:\", 8.85717*x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Verify the left eigenvectors with the eigenvalues:\n",
      "First (left) eigenvector:\n",
      "  x^t A: [ 1.854713 -2.683469  0.80973 ]\n",
      "  l x^t: [ 1.85471142 -2.68347576  0.80973072]\n",
      "Second (left) eigenvector:\n",
      "  x^t A: [-0.254833  -0.0456942  0.4322744]\n",
      "  l x^t: [-0.25483176 -0.0456919   0.43227552]\n",
      "Third (left) eigenvector:\n",
      "  x^t A: [-5.873252 -5.272274 -4.019635]\n",
      "  l x^t: [-5.87325143 -5.27227473 -4.01963175]\n"
     ]
    }
   ],
   "source": [
    "# The eigenvector matrix is orthogonal; the right eigenvectors are also the left eigenvectors\n",
    "print(\"Verify the left eigenvectors with the eigenvalues:\")\n",
    "\n",
    "print(\"First (left) eigenvector:\")\n",
    "x = np.array([-0.551825, 0.798404, -0.240916], dtype='float64')\n",
    "print(\"  x^t A:\", np.dot(x, A))\n",
    "print(\"  l x^t:\", -3.36105*x)\n",
    "\n",
    "print(\"Second (left) eigenvector:\")\n",
    "x = np.array([-0.505745, -0.0906812, 0.857904], dtype='float64')\n",
    "print(\"  x^t A:\", np.dot(x, A))\n",
    "print(\"  l x^t:\", 0.503874*x)\n",
    "\n",
    "print(\"Third (left) eigenvector:\")\n",
    "x = np.array([-0.663107, -0.595255, -0.453828], dtype='float64')\n",
    "print(\"  x^t A:\", np.dot(x, A))\n",
    "print(\"  l x^t:\", 8.85717*x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Singular value decomposition is like eigenvalue problems.  Instead of obtaining the eigenvalue and the eigenvector matrices, SVD is to obtain the singular value and the left and right singular vector matrix\n",
    "\n",
    "\\begin{align*}\n",
    "\\mathrm{A}_{m\\times n} = \\mathrm{U}_{m\\times m}\\mathrm{\\Sigma}_{m\\times n}\\mathrm{V}_{n\\times n}^t\n",
    "\\end{align*}\n",
    "\n",
    "where $\\mathrm{U}$ is the eigenvector matrix of $\\mathrm{A}\\mathrm{A}^t$, $\\mathrm{V}$ the eigenvector matrix of $\\mathrm{A}^t\\mathrm{A}$, and $\\mathrm{\\Sigma}$ a diagonal matrix whose values are the square root of the non-zero eigenvalues of $\\mathrm{A}\\mathrm{A}^t$ or $\\mathrm{A}^t\\mathrm{A}$.\n",
    "\n",
    "The singular values $\\sigma_1, \\sigma_2, \\ldots, \\sigma_r$ of $\\mathrm{A}$ are the diagonal values of $\\mathrm{\\Sigma}$.  In the SVD problem, the matrix $\\mathrm{A}$ may be rectangular instead of square."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use LAPACK's `?GESVD`to compute SVD:\n",
    "\n",
    "```cpp\n",
    "const size_t m = 3, n = 4;\n",
    "int status;\n",
    "\n",
    "std::cout << \">>> SVD\" << std::endl;\n",
    "Matrix mat(m, n, false);\n",
    "mat(0,0) = 3; mat(0,1) = 5; mat(0,2) = 2; mat(0, 3) = 6;\n",
    "mat(1,0) = 2; mat(1,1) = 1; mat(1,2) = 3; mat(1, 3) = 2;\n",
    "mat(2,0) = 4; mat(2,1) = 3; mat(2,2) = 2; mat(2, 3) = 4;\n",
    "std::vector<double> s(m), superb(m);\n",
    "Matrix u(m, m, false);\n",
    "Matrix vt(n, n, false);\n",
    "\n",
    "std::cout << \"A:\" << mat << std::endl;\n",
    "\n",
    "status = LAPACKE_dgesvd(\n",
    "    LAPACK_ROW_MAJOR // int matrix_layout;\n",
    "  , 'A' // char jobu;\n",
    "  , 'A' // char jobvt;\n",
    "  , m // lapack_int m\n",
    "  , n // lapack_int n\n",
    "  , mat.data() // double * a\n",
    "  , mat.ncol() // lapack_int lda\n",
    "  , s.data() // double * s\n",
    "  , u.data() // double * u\n",
    "  , u.ncol() // lapack_int ldu\n",
    "  , vt.data() // double * vt\n",
    "  , vt.ncol() // lapack_int ldvt\n",
    "  , superb.data() // double * superb\n",
    ");\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "g++  -std=c++17 -O3 -g -m64 -I/opt/intel/mkl/include /opt/intel/mkl/lib/libmkl_intel_lp64.a /opt/intel/mkl/lib/libmkl_sequential.a /opt/intel/mkl/lib/libmkl_core.a -lpthread -lm -ldl  -o 11_gesvd 11_gesvd.cpp\n",
      ">>> SVD\n",
      "A:\n",
      "           3          5          2          6\n",
      "           2          1          3          2\n",
      "           4          3          2          4\n",
      " data:  3 5 2 6 2 1 3 2 4 3 2 4\n",
      "dgesvd status: 0\n",
      "singular values:  11.3795 2.45858 1.20947\n",
      "u: \n",
      "   -0.745981  -0.530655   -0.40239\n",
      "   -0.324445   0.817251  -0.476274\n",
      "   -0.581591   0.224738   0.781822\n",
      " data:  -0.745981 -0.530655 -0.40239 -0.324445 0.817251 -0.476274 -0.581591 0.224738 0.781822\n",
      "vt: \n",
      "   -0.458123  -0.509612  -0.318862  -0.654787\n",
      "     0.38294  -0.472553   0.748366  -0.264574\n",
      "    0.799992  -0.118035  -0.553927  -0.198105\n",
      "  -0.0591054  -0.709265  -0.177316   0.679712\n",
      " data:  -0.458123 -0.509612 -0.318862 -0.654787 0.38294 -0.472553 0.748366 -0.264574 0.799992 -0.118035 -0.553927 -0.198105 -0.0591054 -0.709265 -0.177316 0.679712\n"
     ]
    }
   ],
   "source": [
    "!make 11_gesvd; ./11_gesvd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Verify the results:\n",
      "A:\n",
      "array([[3., 5., 2., 6.],\n",
      "       [2., 1., 3., 2.],\n",
      "       [4., 3., 2., 4.]])\n",
      "USV^t:\n",
      "array([[3.00001146, 5.00000567, 2.00000761, 6.00000733],\n",
      "       [2.00000598, 1.00000157, 3.00000366, 2.00000171],\n",
      "       [4.00000932, 3.00000622, 2.00000865, 4.00000809]])\n",
      "error:\n",
      "array([[1.14555417e-05, 5.66863979e-06, 7.61360053e-06, 7.32884776e-06],\n",
      "       [5.97550818e-06, 1.57297897e-06, 3.66276266e-06, 1.71457387e-06],\n",
      "       [9.32111966e-06, 6.21632203e-06, 8.64943021e-06, 8.09395774e-06]])\n"
     ]
    }
   ],
   "source": [
    "import pprint\n",
    "\n",
    "print(\"Verify the results:\")\n",
    "\n",
    "a = np.array(\n",
    "    [\n",
    "        [3, 5, 2, 6],\n",
    "        [2, 1, 3, 2],\n",
    "        [4, 3, 2, 4],\n",
    "    ], dtype='float64')\n",
    "\n",
    "u = np.array(\n",
    "    [\n",
    "        [-0.745981, -0.530655, -0.40239],\n",
    "        [-0.324445,  0.817251, -0.476274],\n",
    "        [-0.581591,  0.224738, 0.781822],\n",
    "    ], dtype='float64')\n",
    "s = np.array(\n",
    "    [\n",
    "        [11.3795,       0,       0, 0],\n",
    "        [      0, 2.45858,       0, 0],\n",
    "        [      0,       0, 1.20947, 0],\n",
    "    ], dtype='float64'\n",
    ")\n",
    "vt = np.array(\n",
    "    [\n",
    "        [-0.458123, -0.509612, -0.318862, -0.654787],\n",
    "        [  0.38294, -0.472553,  0.748366, -0.264574],\n",
    "        [ 0.799992, -0.118035, -0.553927, -0.198105],\n",
    "        [-0.0591054,-0.709265, -0.177316,  0.679712],\n",
    "    ], dtype='float64'\n",
    ")\n",
    "\n",
    "print(\"A:\")\n",
    "pprint.pprint(a)\n",
    "print(\"USV^t:\")\n",
    "pprint.pprint(np.dot(np.dot(u, s), vt))\n",
    "print(\"error:\")\n",
    "pprint.pprint(np.abs(np.dot(np.dot(u, s), vt) - a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keep the 2 most significant singular values:\n",
      "USV^t:\n",
      "array([[3.38935047, 4.94256056, 1.73042318, 5.90359386],\n",
      "       [2.46083266, 0.9320088 , 2.68092004, 1.88588549],\n",
      "       [3.24354468, 3.11161896, 2.52379662, 4.18733425]])\n",
      "error:\n",
      "array([[0.38935047, 0.05743944, 0.26957682, 0.09640614],\n",
      "       [0.46083266, 0.0679912 , 0.31907996, 0.11411451],\n",
      "       [0.75645532, 0.11161896, 0.52379662, 0.18733425]])\n",
      "\n",
      "Keep the 2 least significant singular values:\n",
      "USV^t:\n",
      "array([[-0.88894466,  0.67396506, -0.70677708,  0.441592  ],\n",
      "       [ 0.30860584, -0.88149708,  1.82275818, -0.41748621],\n",
      "       [ 0.96805291, -0.37271546, -0.11028855, -0.33351291]])\n",
      "error:\n",
      "array([[3.88894466, 4.32603494, 2.70677708, 5.558408  ],\n",
      "       [1.69139416, 1.88149708, 1.17724182, 2.41748621],\n",
      "       [3.03194709, 3.37271546, 2.11028855, 4.33351291]])\n"
     ]
    }
   ],
   "source": [
    "print(\"Keep the 2 most significant singular values:\")\n",
    "\n",
    "smost = np.array(\n",
    "    [\n",
    "        [11.3795,       0, 0, 0],\n",
    "        [      0, 2.45858, 0, 0],\n",
    "        [      0,       0, 0, 0],\n",
    "    ], dtype='float64'\n",
    ")\n",
    "rebuilt = np.dot(np.dot(u, smost), vt)\n",
    "print(\"USV^t:\")\n",
    "pprint.pprint(rebuilt)\n",
    "print(\"error:\")\n",
    "pprint.pprint(np.abs(rebuilt - a))\n",
    "print()\n",
    "\n",
    "print(\"Keep the 2 least significant singular values:\")\n",
    "sleast = np.array(\n",
    "    [\n",
    "        [0,       0,       0, 0],\n",
    "        [0, 2.45858,       0, 0],\n",
    "        [0,       0, 1.20947, 0],\n",
    "    ], dtype='float64'\n",
    ")\n",
    "rebuilt = np.dot(np.dot(u, sleast), vt)\n",
    "print(\"USV^t:\")\n",
    "pprint.pprint(rebuilt)\n",
    "print(\"error:\")\n",
    "pprint.pprint(np.abs(rebuilt - a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keep only the most significant singular values:\n",
      "USV^t:\n",
      "array([[3.88895612, 4.32604061, 2.70678469, 5.55841533],\n",
      "       [1.69140014, 1.88149865, 1.17724548, 2.41748793],\n",
      "       [3.03195641, 3.37272167, 2.1102972 , 4.333521  ]])\n",
      "error:\n",
      "array([[0.88895612, 0.67395939, 0.70678469, 0.44158467],\n",
      "       [0.30859986, 0.88149865, 1.82275452, 0.41748793],\n",
      "       [0.96804359, 0.37272167, 0.1102972 , 0.333521  ]])\n"
     ]
    }
   ],
   "source": [
    "print(\"Keep only the most significant singular values:\")\n",
    "\n",
    "s1 = np.array(\n",
    "    [\n",
    "        [11.3795, 0, 0, 0],\n",
    "        [      0, 0, 0, 0],\n",
    "        [      0, 0, 0, 0],\n",
    "    ], dtype='float64'\n",
    ")\n",
    "print(\"USV^t:\")\n",
    "pprint.pprint(np.dot(np.dot(u, s1), vt))\n",
    "print(\"error:\")\n",
    "pprint.pprint(np.abs(np.dot(np.dot(u, s1), vt)-a))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Least square"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LAPACK provides `?GESL` and the associated functions to find the approximated solution of an over- or under-determined system, $\\min|\\mathrm{A}\\mathbf{x}-\\mathbf{b}|$, where $\\mathbf{x}$ is the unknown."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercises\n",
    "\n",
    "1. Extend the class `Matrix` to be an arbitrary dimensional array.\n",
    "2. Develop your own matrix-matrix multiplication code, measure the runtime, and\n",
    "compare with that of BLAS ``DGEMM`` subroutine.  The matrix size should be\n",
    "larger than or equal to :math:`1000\\times1000`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "\n",
    "* BLAS: http://www.netlib.org/blas/\n",
    "* LAPACK: http://www.netlib.org/lapack/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4+"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
